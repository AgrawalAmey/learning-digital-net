{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learning Digital Circuits.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcIa8ZKLtcQZ",
        "colab_type": "text"
      },
      "source": [
        "# Learning Digital Circuits: A Journey Through Weight Invariant Self-Pruning Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSxVndpFttFP",
        "colab_type": "text"
      },
      "source": [
        "This notebook contains source code for paper [\"Learning Digital Circuits: A Journey Through Weight Invariant Self-Pruning Neural Networks\".](https://arxiv.org/pdf/1909.00052.pdf)\n",
        "\n",
        "To cite:\n",
        "\n",
        "```\n",
        "@ARTICLE{2019arXiv190900052A,\n",
        "       author = {{Agrawal}, Amey and {Karlupia}, Rohit},\n",
        "        title = \"{Learning Digital Circuits: A Journey Through Weight Invariant Self-Pruning Neural Networks}\",\n",
        "      journal = {arXiv e-prints},\n",
        "         year = \"2019\",\n",
        "        month = \"Aug\",\n",
        "          eid = {arXiv:1909.00052},\n",
        "archivePrefix = {arXiv},\n",
        "       eprint = {1909.00052}\n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znzg8iz-wpHe",
        "colab_type": "text"
      },
      "source": [
        "### Install Dependecies\n",
        "\n",
        "We need TensorBoardX for PyTorch TensorBoard support and TensorFlow 2 for TensorBoard notebook plugin. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXzrC3a4xtd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorboardx\n",
        "!pip install -q tensorflow==2.0.0-rc0\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbzvtueE2M3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clear old tensorboard logs\n",
        "!rm -rf runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7YnawoTqqYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Function, Variable\n",
        "from torch.distributions.bernoulli import Bernoulli\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torchvision import datasets, transforms\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojh-iafX0poE",
        "colab_type": "text"
      },
      "source": [
        "### Define some useful utilitfunctions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFZ3jQYmOuqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A handy utility which does `x1 if cond else x2` for tensors\n",
        "def where(cond, x1, x2):\n",
        "    return cond.float() * x1 + (1 - cond.float()) * x2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZKz4E6L0pKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add any layers that need to be binarized here\n",
        "def should_binarize(x):\n",
        "    return isinstance(x, nn.Conv2d) or isinstance(x, nn.Linear)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvIdRqKq7CdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A singleton which allows loading writer from anywhere\n",
        "class TensorBoard:\n",
        "    _writer = None\n",
        "\n",
        "    @classmethod\n",
        "    def get_writer(cls):\n",
        "        if cls._writer:\n",
        "            return cls._writer\n",
        "        cls._writer = SummaryWriter()\n",
        "        \n",
        "        return cls._writer\n",
        "\n",
        "    @classmethod\n",
        "    def reset_writer(cls):\n",
        "        cls._writer = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzDGljNn67Gi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prints some info about network layers\n",
        "def print_net_stats():\n",
        "    print(\"--------------------- Network Statistics --------------------------\")\n",
        "    for name, param in net.named_parameters():\n",
        "        print(name)\n",
        "        param = param.abs()\n",
        "        n_elems = param.nelement()\n",
        "        non_zero = torch.sum(where(param >= 0.5, 1, 0)).item()\n",
        "        print(f\"\"\"\\\n",
        "            Number of units: {n_elems},\n",
        "            Mean: {param.mean()},\n",
        "            Std: {param.std()},\n",
        "            Units > 0.5: {non_zero} ({non_zero * 100 / n_elems:.2f}%)\"\"\")\n",
        "        if name.endswith(\"invert\"):\n",
        "            print(name, param.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_z7dJHXxXAI",
        "colab_type": "text"
      },
      "source": [
        "### Define Normalization layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsiNsmlDxfRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HardInvertLayer(nn.Module):   \n",
        "    def forward(self, x):\n",
        "        return 1 - x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYpqAPyaGTIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftInvertLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SoftInvertLayer, self).__init__()\n",
        "        self.invert = nn.Parameter(torch.rand(1))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        self.invert.data.clamp_(0, 1)\n",
        "        return x * (1 - self.invert) + (1 - x) * self.invert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUCIzT1fxhTL",
        "colab_type": "text"
      },
      "source": [
        "### Define helper module to convert regular network to binarized network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLqQ5KP5Ee6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BinaryConnect():\n",
        "    def __init__(self, model, binarize_bias):\n",
        "        self.model = model\n",
        "        self.binarize_bias = binarize_bias\n",
        "\n",
        "    # Add any layers with learnable params in the if\n",
        "    def _get_modules(self):\n",
        "        return [x for x in self.model.modules() \\\n",
        "                if should_binarize(x)]\n",
        "\n",
        "    # For weight cliping\n",
        "    def clip(self, low=0, high=1.0):\n",
        "        [x.weight.data.clamp_(low, high) for x in self._get_modules()]\n",
        "    \n",
        "        if self.binarize_bias:\n",
        "            [x.bias.data.clamp_(low, high) for x in self._get_modules()]\n",
        "    \n",
        "    # Binarizes weights based on > 0.5\n",
        "    def _get_binary(self, x, on, off):\n",
        "        return where(torch.abs(x) >= 0.5, on, off)\n",
        "    \n",
        "    # Call during training loop to convert all weights to binary values \n",
        "    def binarize(self, on, off, random_mag, epsilon):\n",
        "        self._save_params()\n",
        "        \n",
        "        on = on + random.uniform(0, random_mag) if random.random() < epsilon else on\n",
        "\n",
        "        [x.weight.data.copy_(self._get_binary(x.weight.data, on, off)) \\\n",
        "         for x in self._get_modules()]\n",
        "\n",
        "        if self.binarize_bias:\n",
        "            [x.bias.data.copy_(self._get_binary(x.bias.data, on, off)) \\\n",
        "             for x in self._get_modules()]\n",
        "\n",
        "    # Stores weights during binarization so that they can be restored later\n",
        "    def _save_params(self):\n",
        "        self.saved_weights = [x.weight.data.clone() for x in self._get_modules()]\n",
        "\n",
        "        if self.binarize_bias:\n",
        "            self.saved_biases = [x.bias.data.clone() for x in self._get_modules()]\n",
        "\n",
        "    # Restores original weights back\n",
        "    def restore(self):\n",
        "        [x.weight.data.copy_(y) for x, y in zip(self._get_modules(), self.saved_weights)]\n",
        "\n",
        "        if self.binarize_bias:\n",
        "            [x.bias.data.copy_(y) for x, y in zip(self._get_modules(), self.saved_biases)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9G1U3Wy7KBj",
        "colab_type": "text"
      },
      "source": [
        "### Define a simple MLP model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7UVApTPGY1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features,\n",
        "                 out_features,\n",
        "                 hidden_units,\n",
        "                 bias,\n",
        "                 binary,\n",
        "                 norm_type,\n",
        "                 hist_freq):\n",
        "\n",
        "        super(SimpleModel, self).__init__()\n",
        "\n",
        "        # Config Options\n",
        "        self._bias = bias\n",
        "        self._binary = binary\n",
        "        self._norm_type = norm_type\n",
        "        # Tensorboard related variables         \n",
        "        self._num_batches = 0\n",
        "        self._hist_freq = hist_freq\n",
        "        self._writer = TensorBoard.get_writer()\n",
        "\n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(in_features, hidden_units, bias=bias)\n",
        "        self.norm1 = self._get_norm(hidden_units)\n",
        "        \n",
        "        self.fc2 = nn.Linear(hidden_units, hidden_units, bias=bias)\n",
        "        self.norm2 = self._get_norm(hidden_units)\n",
        "        \n",
        "        self.fc3 = nn.Linear(hidden_units, hidden_units, bias=bias)\n",
        "        self.norm3 = self._get_norm(hidden_units)\n",
        "        \n",
        "        self.fc4 = nn.Linear(hidden_units, hidden_units, bias=bias)\n",
        "        self.norm4 = self._get_norm(hidden_units)\n",
        "\n",
        "        self.fcx = nn.Linear(hidden_units, out_features, bias=bias)\n",
        "        self.normx = self._get_norm(out_features)\n",
        "        \n",
        "        self._reset_parameters()\n",
        "\n",
        "    # Returns normalization fuction   \n",
        "    def _get_norm(self, in_features):\n",
        "        if self._norm_type == \"soft_invert\":\n",
        "            return SoftInvertLayer()\n",
        "        elif self._norm_type == \"hard_invert\":\n",
        "            return HardInvertLayer()\n",
        "        else:\n",
        "            return nn.BatchNorm1d(in_features, eps=1e-4, momentum=0.15)\n",
        "        \n",
        "    def _reset_parameters(self):\n",
        "        if self._binary:\n",
        "            [x.weight.data.bernoulli_(0.001) \\\n",
        "             for x in self._modules.values() if should_binarize(x)]\n",
        "\n",
        "        if self._bias:\n",
        "            [x.bias.data.zero_() \\\n",
        "             for x in self._modules.values() if should_binarize(x)]\n",
        "\n",
        "    def _forward_block(self, x, i):\n",
        "        a = torch.tanh(self._modules[f\"fc{i}\"](x))\n",
        "        \n",
        "        x = self._modules[f\"norm{i}\"](a)\n",
        "        \n",
        "        if self._num_batches % self._hist_freq == 0:\n",
        "            self._writer.add_histogram(f\"Activation {i}\",\n",
        "                                      a.data.cpu().numpy(),\n",
        "                                      self._num_batches)\n",
        "            self._writer.add_histogram(f\"Normalized Activation {i}\",\n",
        "                                      x.data.cpu().numpy(),\n",
        "                                      self._num_batches)\n",
        "\n",
        "        return x\n",
        "        \n",
        "    def forward(self, x):\n",
        "        self._num_batches += 1\n",
        "        \n",
        "        for i in range(1, 4):\n",
        "            x = self._forward_block(x, i)\n",
        "\n",
        "        x = self._forward_block(x, 'x')\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deIXAb0ermFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args): \n",
        "    print(\"--------------------- Starting Training ---------------------------\")\n",
        "    \n",
        "    # Set seeds     \n",
        "    torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "    random.seed(0)\n",
        "\n",
        "    # Init model     \n",
        "    net = SimpleModel(args.in_features,\n",
        "                      args.out_features,\n",
        "                      args.hidden_units,\n",
        "                      args.bias,\n",
        "                      args.binary,\n",
        "                      args.norm,\n",
        "                      args.hist_freq)\n",
        "\n",
        "\n",
        "    # Init binarization helper\n",
        "    bc = BinaryConnect(net, args.bias) if args.binary else None\n",
        "    \n",
        "    if args.cuda:\n",
        "        net.cuda()\n",
        "\n",
        "    print(\"--------------------- Training Parameters --------------------------\")\n",
        "    print(args)\n",
        "    print(net)\n",
        "    print(\"--------------------------------------------------------------------\")\n",
        "    \n",
        "\n",
        "    # Init dataset     \n",
        "    dataset_provider = datasets.FashionMNIST if args.dataset == \"fashion\" else datasets.MNIST\n",
        "    \n",
        "    kwargs = {'num_workers': 2, 'pin_memory': True} if args.cuda else {}\n",
        "    \n",
        "    train_loader = data.DataLoader(\n",
        "        dataset_provider('./data', train=True, download=True,\n",
        "                         transform=transforms.ToTensor()),\n",
        "                         batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "    test_loader = data.DataLoader(\n",
        "        dataset_provider('./data', train=False,\n",
        "                         transform=transforms.ToTensor()),\n",
        "                         batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "    # Define optimizer and loss function    \n",
        "    optimizer = optim.Adam(net.parameters(), lr=args.lr)\n",
        "    creterion = nn.NLLLoss()\n",
        "    \n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        if epoch % args.print_freq == 0:\n",
        "            print(f\"Epoch {epoch}:\")\n",
        "\n",
        "        train_epoch(epoch,\n",
        "                    net,\n",
        "                    bc,\n",
        "                    creterion,\n",
        "                    optimizer,\n",
        "                    train_loader,\n",
        "                    args,\n",
        "                    on=1,\n",
        "                    off=0,\n",
        "                    random_mag=0,\n",
        "                    epsilon=1)\n",
        "\n",
        "\n",
        "        if epoch % args.full_eval_freq == 0 and args.binary:\n",
        "            for on in np.arange(0, 4.1, 0.1):\n",
        "                val_epoch(epoch,\n",
        "                          net,\n",
        "                          bc,\n",
        "                          creterion,\n",
        "                          test_loader,\n",
        "                          args,\n",
        "                          on=on,\n",
        "                          off=0,\n",
        "                          random_mag=0,\n",
        "                          epsilon=1)\n",
        "        else:\n",
        "            val_epoch(epoch,\n",
        "                      net,\n",
        "                      bc,\n",
        "                      creterion,\n",
        "                      test_loader,\n",
        "                      args,\n",
        "                      on=1,\n",
        "                      off=0,\n",
        "                      random_mag=0,\n",
        "                      epsilon=1)\n",
        "    \n",
        "    print(\"--------------------- Training Completed --------------------------\")\n",
        "\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJGhzXxygUqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(epoch,\n",
        "                net,\n",
        "                bc,\n",
        "                creterion,\n",
        "                optimizer,\n",
        "                train_loader,\n",
        "                args, \n",
        "                on,\n",
        "                off,\n",
        "                random_mag,\n",
        "                epsilon):\n",
        "    losses = 0\n",
        "    accs = 0\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if args.cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        data, target = Variable(data.view(data.shape[0], -1)), Variable(target)\n",
        "\n",
        "        if args.binary:\n",
        "            # Binarize input\n",
        "            data = where(torch.abs(data) >= 0.5, 1, 0)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        if args.binary:\n",
        "            bc.binarize(on, off, random_mag, epsilon)\n",
        "        \n",
        "        output = net(data)\n",
        "        \n",
        "        loss = creterion(output, target)\n",
        "        loss.backward()\n",
        "        \n",
        "        if args.binary:\n",
        "            bc.restore()\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        if args.binary:\n",
        "            bc.clip()\n",
        "        \n",
        "        y_pred = torch.max(output, 1)[1]\n",
        "        accs += (torch.mean((y_pred == target).float())).item()\n",
        "\n",
        "        losses += loss.item()\n",
        "        \n",
        "    writer = TensorBoard.get_writer()\n",
        "    \n",
        "    writer.add_scalar(\"Train Accuracy\", accs / batch_idx, epoch)\n",
        "    writer.add_scalar(\"Train Loss\", losses / batch_idx, epoch)\n",
        "    \n",
        "    [writer.add_histogram(x[0], x[1].data.cpu().numpy(), epoch) for x in net.named_parameters()]\n",
        "    \n",
        "    if epoch % args.print_freq == 0:\n",
        "        print(\"Train Loss={0:.3f}, Train Accuracy={1:.3f}\".format(losses / batch_idx, accs / batch_idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngv0F-A9gR_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def val_epoch(epoch,\n",
        "              net,\n",
        "              bc,\n",
        "              creterion,\n",
        "              test_loader,\n",
        "              args, \n",
        "              on,\n",
        "              off,\n",
        "              random_mag,\n",
        "              epsilon):\n",
        "\n",
        "    if not args.binary:\n",
        "        net.eval()\n",
        "\n",
        "    if args.binary:\n",
        "        bc.binarize(on, off, random_mag, epsilon)\n",
        "    \n",
        "    losses = 0\n",
        "    accs = 0\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(test_loader, 1):\n",
        "        if args.cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data.view(data.shape[0], -1)), Variable(target)\n",
        "\n",
        "        if args.binary:\n",
        "            data = where(torch.abs(data) >= 0.5, 1, 0)\n",
        "        output = net(data)\n",
        "        loss = creterion(output, target)\n",
        "        \n",
        "        y_pred = torch.max(output, 1)[1]\n",
        "        accs += (torch.mean((y_pred == target).float())).item()\n",
        "        \n",
        "        losses += loss.item()\n",
        "\n",
        "    if args.binary:\n",
        "        bc.restore()\n",
        "    \n",
        "    writer = TensorBoard.get_writer()\n",
        "    \n",
        "    writer.add_scalar(f\"Validation Accuracy ({on:.1f})\", accs / batch_idx, epoch)\n",
        "    writer.add_scalar(f\"Validation Loss ({on:.1f})\", losses / batch_idx, epoch)\n",
        "    \n",
        "    if epoch % args.print_freq == 0:\n",
        "        print(\"    Weight={0:.1f}, Validation Loss={1:.3f}, Validation Accuracy={2:.3f}\"\\\n",
        "              .format(on, losses / batch_idx, accs / batch_idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIyomolXf2j3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_args(argv):\n",
        "    parser = argparse.ArgumentParser(description='Binary Neural Networks')\n",
        "\n",
        "    parser.add_argument('--binary',\n",
        "                        default=False,\n",
        "                        action=\"store_true\",\n",
        "                        help='If to use bianry')\n",
        "    parser.add_argument('--cuda',\n",
        "                        default=False,\n",
        "                        action=\"store_true\",\n",
        "                        help='Use cuda or not')\n",
        "    parser.add_argument('--bias',\n",
        "                        default=False,\n",
        "                        action=\"store_true\",\n",
        "                        help='Use bias')\n",
        "    parser.add_argument('--in_features',\n",
        "                        type=int,\n",
        "                        default=784,\n",
        "                        help='input features dim')\n",
        "    parser.add_argument('--out_features',\n",
        "                        type=int,\n",
        "                        default=10,\n",
        "                        help='Output features dim')\n",
        "    parser.add_argument('--hidden_units',\n",
        "                        type=int,\n",
        "                        default=4000,\n",
        "                        help='Network Hidden Units')\n",
        "    parser.add_argument('--batch_size',\n",
        "                        type=int,\n",
        "                        default=200,\n",
        "                        help='Batch size')\n",
        "    parser.add_argument('--test_batch_size',\n",
        "                        type=int,\n",
        "                        default=1000,\n",
        "                        help='Batch size')\n",
        "    parser.add_argument('--lr',\n",
        "                        type=float,\n",
        "                        default=0.001,\n",
        "                        help='Learning rate')\n",
        "    parser.add_argument('--epochs',\n",
        "                        type=int,\n",
        "                        default=60,\n",
        "                        help='Epochs')\n",
        "    parser.add_argument('--print_freq',\n",
        "                        type=int,\n",
        "                        default=1,\n",
        "                        help='Print frequency')\n",
        "    parser.add_argument('--hist_freq',\n",
        "                        type=int,\n",
        "                        default=100,\n",
        "                        help='Number of batches between activation histogram')\n",
        "    parser.add_argument('--full_eval_freq',\n",
        "                        type=int,\n",
        "                        default=10,\n",
        "                        help='Number of epochs between full evaluation')\n",
        "    parser.add_argument('--norm',\n",
        "                        default='batch_norm',\n",
        "                        choices=['soft_invert', 'hard_invert', 'batch_norm'],\n",
        "                        help='Normalization function')\n",
        "    parser.add_argument('--dataset',\n",
        "                        default='mnist',\n",
        "                        choices=['fashion', 'mnist'],\n",
        "                        help='Dataset')\n",
        "\n",
        "    args = parser.parse_args(argv)\n",
        "    \n",
        "    return args"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h57c-S7y7_Po",
        "colab_type": "text"
      },
      "source": [
        "### Load up TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWhKAcZg79cm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The plugin is kind of buggy, if you want better experiance use ngrok\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNF4XtrB74QR",
        "colab_type": "text"
      },
      "source": [
        "### Start Training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BQ9vgnrr838",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    TensorBoard.reset_writer()\n",
        "    args = parse_args(\"--cuda --binary\".split(\" \"))\n",
        "    net = train(args)\n",
        "    print_net_stats(net)\n",
        "\n",
        "main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeSfU_qkCkWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}